{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f231a884",
   "metadata": {},
   "source": [
    "# Week 5: Finishing the TTR Experiment\n",
    "\n",
    "In this week, we put together everything we've learned so far this semester to tackle the full TTR experiment.\n",
    "\n",
    "1. Removing puncutation with regular expressions\n",
    "    - Here we learn about regular expressions, to help us with the super-important task of removing all punctuation from our texts.\n",
    "\n",
    "2. Iterating through files in a folder\n",
    "    - Here we use the `Path()` function to help load a whole folder of texts and analyze them one-by-one.\n",
    "\n",
    "3. Automatically determining sample size and producing standardized results\n",
    "    - Here we learn how to determine the total length of the shortest text, and then calculate the TTR only of a sample of the full text.\n",
    "\n",
    "4. Writing CSV files\n",
    "\n",
    "    - Here we use `open()` and `.write()` to get Python to spit out spreadsheet files with our results all ready to use!\n",
    "\n",
    "\n",
    "## Links\n",
    "\n",
    "* Melanie Walsh discusses regular expressions in [her chapter on web scraping](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/03-Web-Scraping-Part2.html?highlight=regular%20expressions#regular-expressions). Our discussion is probably a bit gentler, but this is here if you're looking for more explanations.\n",
    "* Walsh also covers opening and saving files in [her chapter on files and character encoding](https://melaniewalsh.github.io/Intro-Cultural-Analytics/02-Python/07-Files-Character-Encoding.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31077dc",
   "metadata": {},
   "source": [
    "# Taking Stock on the TTR Task\n",
    "\n",
    "We now know how to do most of what we need to write code that will quickly and accurately calculate non-standardized and standardized TTRs for a folder full of text files.\n",
    "\n",
    "During Week Three, we learned how to\n",
    "* Load a file\n",
    "* Split it into words\n",
    "* Count the number of words (tokens)\n",
    "\n",
    "In Week Four, we used conditionals and iteration to:\n",
    "* Count unique words (types)\n",
    "\n",
    "Today, we will learn how to:\n",
    "* Remove punctuation for more accurate type counts\n",
    "* Iterate through a folder of text files\n",
    "* Automatically determine our sample size (the total length of the shortest text)\n",
    "* Calculate the standardized TTR for each text file in our folder\n",
    "* Output our results in the form of CSV spreadsheet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bb994",
   "metadata": {},
   "source": [
    "# 1. Removing Punctuation with Regular Expressions\n",
    "\n",
    "Regular expressions — also known by their cooler *nom de guerre* **Regex** — are a whole language of their own. And don't let that term \"regular\" fool you — they are **extremely cool**.  (In mathematics terms, \"regular\" means well-defined.)\n",
    "\n",
    "Note that regular expressions aren't only used in Python. They can be used in all programming languages, and you can even use them in good text editors like Sublime Text. They come in extremely handy once you get a handle on them!\n",
    "\n",
    "Imagine them as a super-sophisticated version of a find-and-replace command. We've already met one of those — the `string.replace()` method. Regular expressions go way, way further. We're only going to explore a tiny fraction of what they can do...\n",
    "\n",
    "Let's explore a scnario where `string.replace()` doesn't exactly get us what we want, and we need something more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4 = open(\"sign-of-four.txt\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4[1475:1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03f8d6",
   "metadata": {},
   "source": [
    "Let's say we would like to do three things to clean up this string:\n",
    "* Replace the too-risqué word \"cocaine\" with \"strawberry soda\"\n",
    "* Remove all punctuation\n",
    "* Extract only the dialogue from this exchange, storing each piece of dialogue as an item in a list\n",
    "\n",
    "Our old friend `string.replace()` can easily do the first, do the second with a lot of effort, and not do the latter at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99258478",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"cocaine\", \"stawberry soda\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95b45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"?\", \"\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a6c71",
   "metadata": {},
   "source": [
    "As you can see, `string.replace()` can remove punctuation... but we need to specify each piece of punctuation one-by-one, which is rather laborious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"“\", \"\")\n",
    "print(cocaine_exchange)\n",
    "cocaine_exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7bba7",
   "metadata": {},
   "source": [
    "As for extracting all passages of dialogue, `string.replace()` offers nothing at all. It can only take particular strings and replace them with other strings.\n",
    "\n",
    "This is where **regular expressions** come in..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259ac45",
   "metadata": {},
   "source": [
    "## Python libraries\n",
    "\n",
    "Python has a bunch of built-in functionality that you need to explicitly call on to \"activate.\" Think of it as a resources issues. Not everyone cares about fancy find-and-replace functions (boring people, to be specific) — so they don't want their Python programs loaded with lots of unnecessary commands they won't call on. There are tons of domain-specific commands (for manipulating files, graphs, images, statistics, etc.) that are probably cool but that we don't intend to use — and we don't want them bogging down our stuff, either.\n",
    "\n",
    "Due to our exquisite taste, we need regular expressions. \n",
    "\n",
    "This requires us to **load a Python library**: a set of commands that are lying politely in wait, waiting for their number to be called, ready to slide down the fireman's pole from the realm of mere potentiality into the world of the actual. The Library we seek is **named `re`.**\n",
    "\n",
    "![Fireman's pole](firemanspole.gif)\n",
    "\n",
    "The command below calls `re` down the fireman's pole. We can now use Regex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd80789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20313c",
   "metadata": {},
   "source": [
    "`re` has a bunch of different **functions** bundled into it, all of whose names begin with `re` then `.` and then the name of the command. This syntax of `re.function()` should be interpreted as calling a function belonging to the `re` library (or module).\n",
    "\n",
    "We're going to start with `re.sub()` which does more or less what `string.replace()` does, though its syntax is a bit different.\n",
    "\n",
    "Since the `string` in `string.replace()` is really a variable containing some text, let's call it `text_variable.replace()` to make it more clear how it differs from `re.sub()`.\n",
    "\n",
    "Recall that `text_variable.replace()` takes who arguments `(\"the string to replace\", \"the string to replace it with\")` and `text_variable` is the variable containing the text to be modified.  In contrast, `re.sub()` takes three arguments: `re.sub(\"the string to replace\", \"the string to replace it with\", the_variable_containing_the_text)`.  \n",
    "\n",
    "Here's how we would do our first two tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a156810",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4[1475:1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93719d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = re.sub(\"cocaine\", \"strawberry soda\", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ff9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = re.sub(\"“\", \"\", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342944e3",
   "metadata": {},
   "source": [
    "Note that **the `re.sub()` function is NOT a mutating function** — so if you want to store its output, you need to explicitly stick it into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "strawberry_soda_exchange = re.sub(\"cocaine\", \"strawberry soda\", cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd91ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(strawberry_soda_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e837a5",
   "metadata": {},
   "source": [
    "## Regular Expresssions\n",
    "\n",
    "Regular Expressions are capable of doing so much more than this.\n",
    "\n",
    "Let's explore some of what you can do with regular expressions [at the website Regular Expressions 101](https://regex101.com).\n",
    "\n",
    "**Don't worry: you don't need to memorize all of this. The only thing you really need to know is how to use regex's to remove punctuation for our TTR task. But we thought you would enjoy seeing a demonstration of regex's immense power!**\n",
    "\n",
    "We'll try the following:\n",
    "\n",
    "* `a`: the character `a`\n",
    "* `[aeiou]`: any one of `a`, `e`, `i`, *or* `u` (the square brackets `[]` mean \"any one of what's between me\")\n",
    "* `[aeiouAEIOU]`: same as above, but adding capital letters\n",
    "* `[a-z]`: any character in the **range** `a-z` — so, any lowercase letter\n",
    "* `[a-zA-Z]`: any lowercase or uppercase letter — so, any letter\n",
    "* `[^a-z]`: anything that is **not** `a-z` (the `^` means \"not\")\n",
    "\n",
    "Then we'll meet these fellows:\n",
    "* `\\w`: any letter\n",
    "* `\\d`: any number\n",
    "* `\\s`: any whitespace\n",
    "* `\\W`: anything *not* a letter\n",
    "* `\\D`: anything *not* a number\n",
    "* `\\S`: anything *not* whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b42d58",
   "metadata": {},
   "source": [
    "## Removing Punctuation with Regular Expressions\n",
    "\n",
    "Our adventures at Regular Expressions 101 will have showed us how we can quite simply remove \"punctuation,\" which we will define as: \n",
    "- **any character that isn't a lowercase letter a-z, an uppercase letter A-Z, or a number 0-9.**\n",
    "\n",
    "In the language of Regex, you would express that same definition as follows: \n",
    "- **[^a-zA-Z0-9]**\n",
    "\n",
    "We want to grab all of those and replace them with **spaces** since replacing the `-` in a word like `seven-per-cent` would turn it into a non-word like `sevenpercent` rather than three separate words, `seven per cent`, which our `string.split()` method will be able to easily \"tokenize.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787ad10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = re.sub(\"[^a-zA-Z0-9]\", \" \", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb61039",
   "metadata": {},
   "source": [
    "**That does the trick!** \n",
    "\n",
    "So now the question becomes: **WHEN** should we remove punctuation? When `sot4` is a string, or after we've used `.split()` to split it into words?\n",
    "\n",
    "Let's try it both ways, first removing punctuation *after* `.split()`ting, and then removing it *before*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_words = cocaine_exchange.split() # First we split the text into words\n",
    "\n",
    "ce_unique_words = []\n",
    "\n",
    "for word in ce_words:\n",
    "    word = word.lower()\n",
    "    word = re.sub(\"[^a-zA-Z0-9]\", \" \", word) # Then we remove punctuation in the for loop that counts unique words\n",
    "    if word not in ce_unique_words:\n",
    "        ce_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498da3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange_nopunct = re.sub(\"[^a-zA-Z0-9]\", \" \", cocaine_exchange) # First we remove punctuation\n",
    "\n",
    "cenp_words = cocaine_exchange_nopunct.split() # Then we split the text into words\n",
    "\n",
    "cenp_unique_words = []\n",
    "\n",
    "for word in cenp_words: # By the time we enter this for loop, the punctuation is already gone\n",
    "    word = word.lower()\n",
    "    if word not in cenp_unique_words:\n",
    "        cenp_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64aad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cenp_unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(ce_unique_words) / len(ce_words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(cenp_unique_words) / len(cenp_words)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66c594",
   "metadata": {},
   "source": [
    "So **we want to remove puncutation *before* using `.split()`,** because otherwise we'll end up with a bunch of funky \"unique words\" with spaces where their punctuation once was. It doesn't solve the problem we initially had. \n",
    "\n",
    "If we remove punctuation before tokenizing with `.split()` we get what we want, because `.split()` splits whenever it meets any number of consecutive whitespace characters. So it will easily turn `asked    morphine` — with its lengthy separating whitespace — into two tokens, `asked` and `morphine`.\n",
    "\n",
    "### Now, let's do this for real\n",
    "\n",
    "We are going to use a different text, because we want to compare TTRs by the end of the class. We have a folder collecting all the Presidential Inaugaral Addresses by U.S. presidents, from George Washington in 1789 to Joe Biden in 2021 (as they are collected [here](https://archive.org/details/Inaugural-Address-Corpus-1789-2009), supplemented from recent ones [here](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses)).\n",
    "\n",
    "We'll start with Washington's 1789 address.  \n",
    "\n",
    "Let's start with how we did it last class, before we knew how to remove punctuation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old method without removing punctuation\n",
    "\n",
    "speech = open('speeches/1789-Washington.txt', encoding='utf-8').read()\n",
    "\n",
    "speech_words = speech.split()\n",
    "\n",
    "speech_unique_words = []\n",
    "\n",
    "for word in speech_words:\n",
    "    word = word.lower()\n",
    "    if word not in speech_unique_words:\n",
    "        speech_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5754bfc",
   "metadata": {},
   "source": [
    "... And then do it our fancy new way, using Regex to remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b790ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New method that removes punctuation\n",
    "\n",
    "speech_np = re.sub(\"[^a-zA-Z0-9]\", \" \", speech) # The variable names here, \"np\" signals \"no punctuation\"\n",
    "\n",
    "speech_np_words = speech_np.split()\n",
    "\n",
    "speech_np_unique_words = []\n",
    "\n",
    "for word in speech_np_words:\n",
    "    word = word.lower()\n",
    "    if word not in speech_np_unique_words:\n",
    "        speech_np_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e14a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897af6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speech_np[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_np_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_unique_words.sort()\n",
    "speech_unique_words[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ae061",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_np_unique_words.sort()\n",
    "speech_np_unique_words[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e26cf",
   "metadata": {},
   "source": [
    "The tokenization without punctuation works a lot better. \n",
    "\n",
    "### How much do you think this will affect the TTR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eced452",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(speech_unique_words) / len(speech_words)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(speech_np_unique_words) / len(speech_np_words)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f908a",
   "metadata": {},
   "source": [
    "# 2. Iterating through Files in a Folder\n",
    "\n",
    "Okay, we now know how to remove punctuation, which is essential to our TTR task: we are now getting accurate token and type counts.\n",
    "\n",
    "We only need to be able to do three more things:\n",
    "* Learn how to automatically create a standardized sample size\n",
    "* Automate the loading of files so that we can can give Python a folder full of text files and let it do its thing, with no additional help from us\n",
    "* Have Python spit out some nice spreadsheet files for us: one with non-standardized values and one with standardized values.\n",
    "\n",
    "First, let's handle the folder-loading task.\n",
    "\n",
    "## The `Path` function\n",
    "\n",
    "For this, we need to pull in another Python Library: `pathlib`, from which we are going to extract the function `Path`. We can coax it down the fireman's pole with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2eefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5dc35",
   "metadata": {},
   "source": [
    "`Path` will help us look through a folder and find the... **pathways** that Python needs to find files we want it to look and calculate TTR values for. \n",
    "\n",
    "First, `Path` needs to know where we've stored our plain text files. We will let it know by passing it a string variable that contains the name of the folder we want it to look in. In this case (check your JupyterHubs!) we've put all the inauguration speeches in a folder called `speeches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"speeches\" # This variable can be named anything as long as is matches the variable name in the Path() command below\n",
    "from pathlib import Path\n",
    "file_list = Path(folder_path).glob(\"*\")\n",
    "sorted(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98f08e",
   "metadata": {},
   "source": [
    "The below command asks `Path` to look in a folder called `speeches` (which it expects to find **in the same folder as the Jupyter Notebook we are currently using — which it is!**), and to print out the paths of **absolutely everything in that folder** (the `\"*\"` as the argument to the `.glob()` method is what instructs it to look for everything)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e40343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in Path(folder_path).glob(\"*\"):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ddc46a",
   "metadata": {},
   "source": [
    "Each of those things is a **file path**: a path or route that Python will need to follow — relative to the place from which it's receiving its commands; namely, this Jupyter Notebook — to get to the files we want it to analyze.\n",
    "\n",
    "As you can see, I have deviously inserted the dreaded `firemanspole.gif` into that folder. We do not want to calculate the TTR of `firemanspole.gif`!! Luckily we can tell `Path` to only look for particular kinds of files. In this case, we only want it to look at plain text files, which all end `.txt`. So we can replace the `*` in the above command (\"I want paths of everything in that folder\") with `*.txt` (\"I only want paths to the plain text files\").\n",
    "\n",
    "(NOTE: The `*` in the `Path().glob()` method means something different from the `*` in Regex. Such is life in the world of computer programming, where one must learn to speak multiple dialects to communicate...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in Path(folder_path).glob('*.txt'):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78644b73",
   "metadata": {},
   "source": [
    "As you'll notice, the above list of files is not sorted, which is both aesthetically annoying and will also make it more difficult to interpret our results. (We can always sort things later using Microsoft Excel, etc., but we may as well do as much as we can right here in Python.)\n",
    "\n",
    "Thankfully, we can wrap the Python function `sorted()` around the `Path()` function, and it will open the files in an alphabetically sorted manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fce68",
   "metadata": {},
   "source": [
    "Now, rather than just spewing out the file paths with the `print()` function... Let's actually **load** each of these files, and print out the first 100 characters of each, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696ce37",
   "metadata": {},
   "source": [
    "Now that we can do this, we can take a major step: we can load an entire folder of files and, for each one, calculate its overall TTR. The code below does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types} types, {tokens} tokens, and a TTR of {ttr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd8971",
   "metadata": {},
   "source": [
    "# 3. Automatically Determining Sample Size and Producing Standardized Results\n",
    "\n",
    "So we're done, right??\n",
    "\n",
    "### **NO!**\n",
    "\n",
    "Why not? What do we still need to be able to do?\n",
    "\n",
    "That's right: calculate *standardized* TTRs!\n",
    "\n",
    "To be specific, we still need to figure out\n",
    "* How to automatically determine our sample size (i.e., the total length of the shortest text)\n",
    "* And then how to calculate TTRs for that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113b2cf",
   "metadata": {},
   "source": [
    "## Calculating the Total Length of the Shortest Item in a List\n",
    "\n",
    "Before we do this with full-length texts, let's try with a small-scale experiment.\n",
    "\n",
    "Let's say create a list with a bunch of strings in it. How could we automatically determine the total length of the shortest of these strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch_o_strings = [\"Marta\", \"Rosie\", \"Jazz\", \"Adamillo\", \"Anna\", \"Stephen\", \"Richard\", \"Ernest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b599d108",
   "metadata": {},
   "source": [
    "There are surely a bunch of ways to do it. But let's use this method.\n",
    "1. Create a new variable called `length_of_shortest_string` where we'll record the length of the shortest string. We'll initially set its value to `0`, because that's the minimum length that a string (or a list) can be.\n",
    "2. Iterate through all the strings in the list using a `for` loop.\n",
    "3. For each item in `bag_o_strings`, grab its length and store it in `string_length`.\n",
    "4. Check whether that `string_length` is the shortest we've seen so far, in which case we'll save that information is `length_of_shortest_string` — otherwise, we'll ignore it. **OR,** if `length_of_shortest_string` is set to 0 — its initial value — we'll put whatever is in `string_length` in there, since that means this is our first time through the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_shortest_string = 0 \n",
    "\n",
    "for string in bunch_o_strings:\n",
    "    string_length= len(string)\n",
    "    if length_of_shortest_string == 0 or string_length < length_of_shortest_string:\n",
    "        length_of_shortest_string = string_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_shortest_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d2e6b",
   "metadata": {},
   "source": [
    "## Creating Standardized Sample Sizes\n",
    "\n",
    "Let's stick with our toy example for a moment.\n",
    "\n",
    "We know that the `sample_size` we want is 4. So how to we actually *slice off* just the first 4 characters of each of those strings?\n",
    "\n",
    "I'll show you the code, you tell me what the comment line does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in bunch_o_strings:\n",
    "    string_standardized = string[:length_of_shortest_string] # What's happening here?\n",
    "    print(string_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8afffe",
   "metadata": {},
   "source": [
    "That's right: our old friend **slicing** comes to our rescure here. Remember that the syntax for slicing is `[start:stop:step]`. If we only want the first four character of a string, we write `string[:4]`. Since we won't know our sample size until we load all the files in a folder, we don't want to hard-code a number in there. So we can use a variable name instead.\n",
    "\n",
    "Now, we're going to be standardizing a sample of `lists` (texts broken up into words) rather than `strings`... but as we know, slicing lists and strings works exactly the same way. If we want the first for items of a list, we would also use `list[:4]`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2d1d4",
   "metadata": {},
   "source": [
    "## We now have ALMOST all the skills we need to quickly and accurately calculate TTRs for any number of plain text files stored together in a folder\n",
    "\n",
    "We already know how to:\n",
    "* Automatically load files in a folder, one-by-one, as strings\n",
    "* Remove punctuation, split them into words, and calculate their total number of words (tokens)\n",
    "* Record the number of unique words (types) with a for loop that also lowercases all words\n",
    "* Automatically standardize the sample size, looking only at the first x words in each text, where x is the total length of the shortest text\n",
    "\n",
    "The only things we still don't know how to do are:\n",
    "* Output the results of our analysis of the total texts (non-standardized results)\n",
    "* Output the standardized results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20a3ff",
   "metadata": {},
   "source": [
    "Below I've written out the code that does all the things we've learned so far. It doesn't store the results anywhere yet, but it does calculate them and print them out.\n",
    "\n",
    "This first cell records information for overall, non-standardized values and also determines our sample size (the total length of the shortest text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474fb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "sample_size = 0  # Note this line and figure out what it's doing!\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    if sample_size == 0 or tokens < sample_size: # The line I noted above connects to this one and the next\n",
    "        sample_size = tokens\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types} types, {tokens} tokens, and a TTR of {ttr}\")\n",
    "    print(f\"So far, the shortest text is {sample_size} words in length.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f873eb",
   "metadata": {},
   "source": [
    "The cell below uses the sample size determined in the previous step (1769 words; stored in the `sample_size` variable) to calculate the standardized TTR for the first 1769 words of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    text_words_standardized = text_words[:sample_size] # This is the key new line in this block of code. Figure out what it does! Where did we define the variable sample_size?\n",
    "    tokens_standardized = len(text_words_standardized)\n",
    "\n",
    "    unique_words_standardized = []\n",
    "    \n",
    "    for word in text_words_standardized:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words_standardized:\n",
    "            unique_words_standardized.append(word)\n",
    "            \n",
    "    types_standardized = len(unique_words_standardized)\n",
    "    \n",
    "    ttr_standardized = (types_standardized / tokens_standardized) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types_standardized} types in the standardized sample of {tokens_standardized} tokens, and a TTR of {ttr_standardized:.2f}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16015122",
   "metadata": {},
   "source": [
    "# 4. Writing CSV files\n",
    "\n",
    "Okay — that's a lot to read through and think about. But let's just get this beast of a TTR task finished by taking the final step: doing all this but also storing the results in a file.\n",
    "\n",
    "We'll do this with the `open()` function, the `open.write()` method, and the CSV file format.\n",
    "\n",
    "## What is a CSV?\n",
    "\n",
    "A CSV is a very simple file format for spreadsheets. The name stands for \"Comma Separated Values\" — and that's really all it is: a plain text file in which values (whatever would go into a cell in a spreadsheet) are separated by commas! The end of a row in a CSV file is signalled by the newline character `\\n`. Text cells should be wrapped in quotation marks (`\"\"`) just like strings in Python. Other than that, no tricks!\n",
    "\n",
    "A pretty table like the following:\n",
    "\n",
    "| Types | Tokens                                                                                  |\n",
    "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
    "| 12         | 24                                                                             |\n",
    "| 33         | 100                                              |\n",
    "| 75 | 500\n",
    "\n",
    "Would be expressed in a CSV file as:\n",
    "\n",
    "```\n",
    "\"Types\",\"Tokens\"\n",
    "12,24\n",
    "33,100\n",
    "75,500\n",
    "```\n",
    "\n",
    "... and a spreadhseet program like Excel would see that and know exactly what to do with it.\n",
    "\n",
    "(By the way, when Quercus gives your final grades at the end of the year to be uploaded to the eMarks system... it will be in the form of a CSV file!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f9edc",
   "metadata": {},
   "source": [
    "## Writing files with `open.write()`\n",
    "\n",
    "We already know how to **load** a file into Python and stick it into a variable. We do it like this, with the `open()` function and the `.read()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4_file = open(\"sign-of-four.txt\", encoding=\"utf-8\")\n",
    "sot4 = sot4_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71652732",
   "metadata": {},
   "source": [
    "Creating and writing to a file in Python happens almost the same wasy:\n",
    "* First, you **`open()`** a file. Because it's not a file that already exists — it's one you're creating out of nothing — you need to set the *argument* `mode` to `\"w\"` (write) rather than the default `\"r\"` (read).\n",
    "* Then you use the `.write()` method to write something into that file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c73d0",
   "metadata": {},
   "source": [
    "\n",
    "1. First, we `open()` a file and assign that \"file object\" to a variable called `file`. Because we want to write to it you need to set the *argument* `mode` to `\"w\"` (write) rather than the default `\"r\"` (read).\n",
    "2. We then write whatever we want into that `file` varable, applying the `.write()` method. We can do this as many times as we like.\n",
    "3. Then we \"close\" our file using the `.close()` method.  This final step is necessary to ensure that the data actually gets stored in the file. It tells the system that we are finished working with this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"a-new-file.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write(\"Here's some text\\n\")\n",
    "file.write(\"Here's some more!\\n\")\n",
    "file.write(\"And that's all I want to write for now!\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"a-new-file.txt\", mode=\"r\", encoding=\"utf-8\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc67e8",
   "metadata": {},
   "source": [
    "Thankfully, this all works exactly the same way for a CSV file. Except... we put in commas between values and wrap text in `\"\"`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19669346",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"my-first-spreadsheet.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Types\",\"Tokens\"\\n') # Note that if you want to write double quotes, you have to wrap them in single quotes, or Python will get confused.\n",
    "file.write(\"12,24\\n\")\n",
    "file.write(\"33,100\\n\")\n",
    "file.write(\"75,500\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd76fa",
   "metadata": {},
   "source": [
    "Now that we know how to write CSV files, we are well and truly finished with the TTR task (well... as much as anything is every finished! I can imagine a few ways to improve it, still. Can you??)\n",
    "\n",
    "Here is the whole process, in a single cell. The major part of your Lab this week is to comment this big block of code, line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "sample_size = 0\n",
    "\n",
    "file = open(\"ttr-overall.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Text\",\"Types\",\"Tokens\",\"TTR\"\\n')\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    print(file_path)\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    if sample_size == 0 or tokens < sample_size:\n",
    "        sample_size = tokens\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    file.write(f'\"{file_path.stem}\",{types},{tokens},{ttr}\\n')\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"ttr-standardized.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Text\",\"Types\",\"Tokens\",\"TTR\"\\n')\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    text_words_standardized = text_words[:sample_size]\n",
    "    tokens_standardized = len(text_words_standardized)\n",
    "\n",
    "    unique_words_standardized = []\n",
    "    \n",
    "    for word in text_words_standardized:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words_standardized:\n",
    "            unique_words_standardized.append(word)\n",
    "            \n",
    "    types_standardized = len(unique_words_standardized)\n",
    "    \n",
    "    ttr_standardized = (types_standardized / tokens_standardized) * 100\n",
    "    \n",
    "    file.write(f'\"{file_path.stem}\",{types_standardized},{tokens_standardized},{ttr_standardized}\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' One final step.  I asked ChatGPT to make me a graph of this data so that we could talk about it. After some back and forth to refine the output, here is the\n",
    "code it gave me.  We will talk about many parts of this code in the remaining weeks of this course.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "# Load the data again\n",
    "df = pd.read_csv(\"ttr-standardized.csv\")\n",
    "\n",
    "# Extract president names from the Text column\n",
    "df[\"President\"] = df[\"Text\"].str.split(\"-\", n=1).str[1]\n",
    "\n",
    "# Create distinct colors for each president\n",
    "unique_presidents = df[\"President\"].unique()\n",
    "cmap = cm.tab20  # any categorical colormap works (tab10, Set3, Paired, etc.)\n",
    "colors = cmap(np.linspace(0, 1, len(unique_presidents)))\n",
    "\n",
    "# Map each president to a color\n",
    "color_map = {pres: colors[i] for i, pres in enumerate(unique_presidents)}\n",
    "bar_colors = df[\"President\"].map(color_map)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "# Bar plot\n",
    "x = np.arange(len(df))\n",
    "plt.bar(x, df[\"TTR\"], color=bar_colors)\n",
    "\n",
    "# Add a linear trend line\n",
    "z = np.polyfit(x, df[\"TTR\"], 1)   # degree 1 polynomial (line)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, p(x), \"k--\", linewidth=2, label=\"Trend line\")\n",
    "\n",
    "# X-axis setup\n",
    "plt.xticks(ticks=x, labels=df[\"Text\"], rotation=90, fontsize=8)\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.ylabel(\"TTR (Type-Token Ratio)\")\n",
    "plt.title(\"Type-Token Ratios of Speeches (Bar Graph, Color-coded by President + Trend Line)\")\n",
    "\n",
    "# Add legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=color_map[pres]) for pres in unique_presidents]\n",
    "plt.legend(handles + [plt.Line2D([0], [0], color=\"k\", linestyle=\"--\")],\n",
    "           list(unique_presidents) + [\"Trend line\"],\n",
    "           bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

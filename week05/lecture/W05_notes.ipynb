{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f231a884",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5: Finishing the TTR Experiment\n",
    "\n",
    "In this week, we put together everything we've learned so far this semester to tackle the full TTR experiment.\n",
    "\n",
    "1. Removing puncutation with regular expressions\n",
    "\n",
    "2. Iterating through files in a folder\n",
    "\n",
    "3. Automatically determining sample size and producing standardized results\n",
    "\n",
    "4. Writing CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bb994",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Removing Punctuation \n",
    "\n",
    "What we want a sophisticated \"find and replace\" operation. Let's start simple with `string.replace()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4 = open(\"sign-of-four.txt\", encoding=\"utf-8\").read()\n",
    "cocaine_exchange = sot4[1475:1843]\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03f8d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we would like to do three things to clean up this string:\n",
    "* Replace the too-risqué word \"cocaine\" with \"strawberry soda\"\n",
    "* Remove all punctuation\n",
    "* Extract only the dialogue from this exchange, storing each piece of dialogue as an item in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99258478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"cocaine\", \"stawberry soda\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95b45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"?\", \"\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a6c71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, `string.replace()` can remove punctuation... but we need to specify each piece of punctuation one-by-one, which is rather laborious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = cocaine_exchange.replace(\"“\", \"\")\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7bba7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As for extracting all passages of dialogue, `string.replace()` offers nothing at all. It can only take particular strings and replace them with other strings.\n",
    "\n",
    "This is where **regular expressions** come in..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259ac45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python libraries\n",
    "\n",
    "Python has a bunch of built-in functionality in libraries or modules that we need to import to be able to use.\n",
    "\n",
    "The regular expression library is named \"re\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd80789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20313c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using library functions\n",
    "\n",
    "`re.sub()` takes three arguments: `re.sub(\"the string to replace\", \"the string to replace it with\", the_variable_containing_the_text)`. \n",
    "\n",
    "Note that **the `re.sub()` function is NOT a mutating function** — so if you want to store its output, you need to explicitly stick it into a variable.\n",
    "\n",
    "Here's how we would do our first two tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93719d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# reset our text\n",
    "cocaine_exchange = sot4[1475:1843]\n",
    "\n",
    "cocaine_exchange = re.sub(\"cocaine\", \"strawberry soda\", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ff9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine_exchange = re.sub(\"“\", \"\", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e837a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regular Expresssions\n",
    "\n",
    "Regular Expressions are capable of doing so much more than this.\n",
    "\n",
    "Let's explore some of what you can do with regular expressions [at the website Regular Expressions 101](https://regex101.com).\n",
    "\n",
    "**Don't worry: you don't need to memorize all of this. The only thing you really need to know is how to use regex's to remove punctuation for our TTR task. But we thought you would enjoy seeing a demonstration of regex's immense power!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b42d58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Removing Punctuation with Regular Expressions\n",
    "\n",
    "Our adventures at Regular Expressions 101 will have showed us how we can quite simply remove \"punctuation,\" which we will define as: \n",
    "- **any character that isn't a lowercase letter a-z, an uppercase letter A-Z, or a number 0-9.**\n",
    "\n",
    "In the language of Regex, you would express that same definition as follows: \n",
    "- **[^a-zA-Z0-9]**\n",
    "\n",
    "We want to grab all of those and replace them with **spaces** since replacing the `-` in a word like `seven-per-cent` would turn it into a non-word like `sevenpercent` rather than three separate words, `seven per cent`, which our `string.split()` method will be able to easily \"tokenize.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786377c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = sot4[1475:1843]\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787ad10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocaine_exchange = re.sub(\"[^a-zA-Z0-9]\", \" \", cocaine_exchange)\n",
    "print(cocaine_exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb61039",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**That does the trick!** \n",
    "\n",
    "So now the question becomes: **WHEN** should we remove punctuation? When `sot4` is a string, or after we've used `.split()` to split it into words?\n",
    "\n",
    "See the lecture notes where we try it both ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c334c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # First we remove punctuation\n",
    "cocaine_exchange_nopunct = re.sub(\"[^a-zA-Z0-9]\", \" \", cocaine_exchange)\n",
    "\n",
    "# Then we split the text into words\n",
    "cenp_words = cocaine_exchange_nopunct.split() \n",
    "\n",
    "cenp_unique_words = []\n",
    "\n",
    "# By the time we enter this for loop, the punctuation is already gone\n",
    "for word in cenp_words: \n",
    "    word = word.lower()\n",
    "    if word not in cenp_unique_words:\n",
    "        cenp_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64aad0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cenp_unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(cenp_unique_words) / len(cenp_words)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66c594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now, let's do this for real\n",
    "\n",
    "We are going to use a different text, because we want to compare TTRs by the end of the class. We have a folder collecting all the Presidential Inaugaral Addresses by U.S. presidents, from George Washington in 1789 to Joe Biden in 2021 (as they are collected [here](https://archive.org/details/Inaugural-Address-Corpus-1789-2009), supplemented from recent ones [here](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses)).\n",
    "\n",
    "We'll start with Washington's 1789 address, using Regex to remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b790ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# New method that removes punctuation\n",
    "speech = open('speeches/1789-Washington.txt', encoding='utf-8').read()\n",
    "\n",
    "# The variable names here, \"np\" signals \"no punctuation\"\n",
    "speech_np = re.sub(\"[^a-zA-Z0-9]\", \" \", speech) \n",
    "\n",
    "speech_np_words = speech_np.split()\n",
    "\n",
    "speech_np_unique_words = []\n",
    "\n",
    "for word in speech_np_words:\n",
    "    word = word.lower()\n",
    "    if word not in speech_np_unique_words:\n",
    "        speech_np_unique_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e14a92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "speech[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897af6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speech_np[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ae061",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_np_unique_words.sort()\n",
    "speech_np_unique_words[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e26cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The tokenization without punctuation works a lot better. \n",
    "\n",
    "### How much do you think this will affect the TTR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without removing the punctuation the TTR is 44.23\n",
    "\n",
    "# When we remove the punctuation..\n",
    "(len(speech_np_unique_words) / len(speech_np_words)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44aa84b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Iterating through Files in a Folder\n",
    "\n",
    "Okay, we now know how to remove punctuation.\n",
    "\n",
    "We need to be able to do three more things:\n",
    "* Automatically create a standardized sample size\n",
    "* Automate the loading of files so that we can can give Python a folder full of text files and let it do its thing, with no additional help from us\n",
    "* Have Python spit out some nice spreadsheet files for us: one with non-standardized values and one with standardized values.\n",
    "\n",
    "First, let's handle the folder-loading task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f908a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The `Path` function\n",
    "\n",
    "We need another Python Library: `pathlib`, from which we are going to extract the function `Path`. We can coax it down the fireman's pole with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2eefd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5dc35",
   "metadata": {},
   "source": [
    "`Path` will help us look through a folder and find the... **pathways** that Python needs to find files we want it to look and calculate TTR values for. \n",
    "\n",
    "First, `Path` needs to know the name of the folder that contains the files. Check for the folder `speeches` in Jupyter Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae855b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# This variable can be named anything as long as is matches the variable name in the Path() command below\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "file_list = Path(folder_path).glob(\"*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98f08e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The below command asks `Path` to look in a folder called `speeches` (which it expects to find **in the same folder as the Jupyter Notebook we are currently using — which it is!**), and to print out the paths of **absolutely everything in that folder** (the `\"*\"` as the argument to the `.glob()` method is what instructs it to look for everything)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e40343",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in Path(folder_path).glob(\"*\"):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1a2ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for file_path in Path(folder_path).glob('*.txt'):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78644b73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you'll notice, the above list of files is not sorted, which is both aesthetically annoying and will also make it more difficult to interpret our results. (We can always sort things later using Microsoft Excel, etc., but we may as well do as much as we can right here in Python.)\n",
    "\n",
    "Thankfully, we can wrap the Python function `sorted()` around the `Path()` function, and it will open the files in an alphabetically sorted manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fce68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, rather than just spewing out the file paths with the `print()` function... Let's actually **load** each of these files, and print out the first 100 characters of each, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696ce37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we can do this, we can take a major step: we can load an entire folder of files and, for each one, calculate its overall TTR. The code below does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types} types, {tokens} tokens, and a TTR of {ttr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd8971",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Automatically Determining Sample Size and Producing Standardized Results\n",
    "\n",
    "To be specific, we still need to figure out\n",
    "* How to automatically determine our sample size (i.e., the total length of the shortest text)\n",
    "* And then how to calculate TTRs for that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113b2cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating the Total Length of the Shortest Item in a List\n",
    "\n",
    "Before we do this with full-length texts, let's try with a small-scale experiment.\n",
    "\n",
    "Let's say create a list with a bunch of strings in it. How could we automatically determine the total length of the shortest of these strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147b9da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bunch_o_strings = [\"Marta\", \"Rosie\", \"Jazz\", \"Adamillo\", \"Anna\", \"Stephen\", \"Richard\", \"Ernest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709dab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the code with your colleagues around you\n",
    "\n",
    "length_of_shortest_string = 0 \n",
    "\n",
    "for string in bunch_o_strings:\n",
    "    string_length= len(string)\n",
    "    if length_of_shortest_string == 0 or string_length < length_of_shortest_string:\n",
    "        length_of_shortest_string = string_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_shortest_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d2e6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Standardized Sample Sizes\n",
    "\n",
    "Let's stick with our toy example for a moment.\n",
    "\n",
    "We know that the `sample_size` we want is 4. So how to we actually *slice off* just the first 4 characters of each of those strings?\n",
    "\n",
    "I'll show you the code, you tell me what the comment line does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c7e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in bunch_o_strings:\n",
    "    string_standardized = string[:length_of_shortest_string] # What's happening here?\n",
    "    print(string_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711145a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We now have ALMOST all the skills we need to quickly and accurately calculate TTRs for any number of plain text files stored together in a folder\n",
    "\n",
    "We already know how to:\n",
    "* Automatically load files in a folder, one-by-one, as strings\n",
    "* Remove punctuation, split them into words, and calculate their total number of words (tokens)\n",
    "* Record the number of unique words (types) with a for loop that also lowercases all words\n",
    "* Automatically standardize the sample size, looking only at the first x words in each text, where x is the total length of the shortest text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2d1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The only things we still don't know how to do are:\n",
    "* Output the results of our analysis of the total texts (non-standardized results)\n",
    "* Output the standardized results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20a3ff",
   "metadata": {},
   "source": [
    "Below I've written out the code that does all the things we've learned so far. It doesn't store the results anywhere yet, but it does calculate them and print them out.\n",
    "\n",
    "This first cell records information for overall, non-standardized values and also determines our sample size (the total length of the shortest text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474fb41",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "sample_size = 0  # Note this line and figure out what it's doing!\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    if sample_size == 0 or tokens < sample_size: # The line I noted above connects to this one and the next\n",
    "        sample_size = tokens\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types} types, {tokens} tokens, and a TTR of {ttr}\")\n",
    "    print(f\"So far, the shortest text is {sample_size} words in length.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f873eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The cell below uses the sample size determined in the previous step (1769 words; stored in the `sample_size` variable) to calculate the standardized TTR for the first 1769 words of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    text_words_standardized = text_words[:sample_size] # This is the key new line in this block of code. Figure out what it does! Where did we define the variable sample_size?\n",
    "    tokens_standardized = len(text_words_standardized)\n",
    "\n",
    "    unique_words_standardized = []\n",
    "    \n",
    "    for word in text_words_standardized:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words_standardized:\n",
    "            unique_words_standardized.append(word)\n",
    "            \n",
    "    types_standardized = len(unique_words_standardized)\n",
    "    \n",
    "    ttr_standardized = (types_standardized / tokens_standardized) * 100\n",
    "    \n",
    "    print(f\"'{file_path.stem}' has {types_standardized} types in the standardized sample of {tokens_standardized} tokens, and a TTR of {ttr_standardized:.2f}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16015122",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Writing CSV files\n",
    "\n",
    "Now let's complete the task by storing the results in a file.\n",
    "\n",
    "We'll do this with the `open()` function, the `open.write()` method, and the CSV file format.\n",
    "\n",
    "A CSV is a very simple file format for spreadsheets. \n",
    "\n",
    "A pretty table like the following:\n",
    "\n",
    "| Types | Tokens                                                                                  |\n",
    "|:-------------:|:---------------------------------------------------------------------------------------------------:|\n",
    "| 12         | 24                                                                             |\n",
    "| 33         | 100                                              |\n",
    "| 75 | 500\n",
    "\n",
    "Would be expressed in a CSV file as:\n",
    "\n",
    "```\n",
    "\"Types\",\"Tokens\"\n",
    "12,24\n",
    "33,100\n",
    "75,500\n",
    "```\n",
    "\n",
    "... and a spreadhseet program like Excel would see that and know exactly what to do with it.\n",
    "\n",
    "(By the way, when Quercus gives your final grades at the end of the year to be uploaded to the eMarks system... it will be in the form of a CSV file!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f9edc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Writing files with `open.write()`\n",
    "\n",
    "We already know how to **load** a file into Python and stick it into a variable. We do it like this, with the `open()` function and the `.read()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sot4_file = open(\"sign-of-four.txt\", encoding=\"utf-8\")\n",
    "sot4 = sot4_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71652732",
   "metadata": {},
   "source": [
    "Creating and writing to a file in Python happens almost the same wasy:\n",
    "* First, you **`open()`** a file. Because it's not a file that already exists — it's one you're creating out of nothing — you need to set the *argument* `mode` to `\"w\"` (write) rather than the default `\"r\"` (read).\n",
    "* Then you use the `.write()` method to write something into that file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c73d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "1. First, we `open()` a file and assign that \"file object\" to a variable called `file`. Because we want to write to it you need to set the *argument* `mode` to `\"w\"` (write) rather than the default `\"r\"` (read).\n",
    "2. We then write whatever we want into that `file` varable, applying the `.write()` method. We can do this as many times as we like.\n",
    "3. Then we \"close\" our file using the `.close()` method.  This final step is necessary to ensure that the data actually gets stored in the file. It tells the system that we are finished working with this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fc233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "file = open(\"a-new-file.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write(\"Here's some text\\n\")\n",
    "file.write(\"Here's some more!\\n\")\n",
    "file.write(\"And that's all I want to write for now!\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"a-new-file.txt\", mode=\"r\", encoding=\"utf-8\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc67e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thankfully, this all works exactly the same way for a CSV file. Except... we put in commas between values and wrap text in `\"\"`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19669346",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"my-first-spreadsheet.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Types\",\"Tokens\"\\n') # Note that if you want to write double quotes, you have to wrap them in single quotes, or Python will get confused.\n",
    "file.write(\"12,24\\n\")\n",
    "file.write(\"33,100\\n\")\n",
    "file.write(\"75,500\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd76fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we know how to write CSV files, we are well and truly finished with the TTR task (well... as much as anything is every finished! I can imagine a few ways to improve it, still. Can you??)\n",
    "\n",
    "Here is the whole process, in a single cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "folder_path = \"speeches\"\n",
    "\n",
    "sample_size = 0\n",
    "\n",
    "file = open(\"ttr-overall.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Text\",\"Types\",\"Tokens\",\"TTR\"\\n')\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    print(file_path)\n",
    "    \n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    tokens = len(text_words)\n",
    "    \n",
    "    if sample_size == 0 or tokens < sample_size:\n",
    "        sample_size = tokens\n",
    "    \n",
    "    unique_words = []\n",
    "    \n",
    "    for word in text_words:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "    types = len(unique_words)\n",
    "    \n",
    "    ttr = (types / tokens) * 100\n",
    "    \n",
    "    file.write(f'\"{file_path.stem}\",{types},{tokens},{ttr}\\n')\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"ttr-standardized.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "file.write('\"Text\",\"Types\",\"Tokens\",\"TTR\"\\n')\n",
    "\n",
    "for file_path in sorted(Path(folder_path).glob('*.txt')):\n",
    "    text = open(file_path, encoding='utf-8').read()\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    text_words = text.split()\n",
    "    text_words_standardized = text_words[:sample_size]\n",
    "    tokens_standardized = len(text_words_standardized)\n",
    "\n",
    "    unique_words_standardized = []\n",
    "    \n",
    "    for word in text_words_standardized:\n",
    "        word = word.lower()\n",
    "        if word not in unique_words_standardized:\n",
    "            unique_words_standardized.append(word)\n",
    "            \n",
    "    types_standardized = len(unique_words_standardized)\n",
    "    \n",
    "    ttr_standardized = (types_standardized / tokens_standardized) * 100\n",
    "    \n",
    "    file.write(f'\"{file_path.stem}\",{types_standardized},{tokens_standardized},{ttr_standardized}\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8ff6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "''' One final step.  I asked ChatGPT to make me a graph of this data so that we could talk about it. After some back and forth to refine the output, here is the\n",
    "code it gave me.  We will talk about many parts of this code in the remaining weeks of this course.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "# Load the data again\n",
    "df = pd.read_csv(\"ttr-standardized.csv\")\n",
    "\n",
    "# Extract president names from the Text column\n",
    "df[\"President\"] = df[\"Text\"].str.split(\"-\", n=1).str[1]\n",
    "\n",
    "# Create distinct colors for each president\n",
    "unique_presidents = df[\"President\"].unique()\n",
    "cmap = cm.tab20  # any categorical colormap works (tab10, Set3, Paired, etc.)\n",
    "colors = cmap(np.linspace(0, 1, len(unique_presidents)))\n",
    "\n",
    "# Map each president to a color\n",
    "color_map = {pres: colors[i] for i, pres in enumerate(unique_presidents)}\n",
    "bar_colors = df[\"President\"].map(color_map)\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "# Bar plot\n",
    "x = np.arange(len(df))\n",
    "plt.bar(x, df[\"TTR\"], color=bar_colors)\n",
    "\n",
    "# Add a linear trend line\n",
    "z = np.polyfit(x, df[\"TTR\"], 1)   # degree 1 polynomial (line)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, p(x), \"k--\", linewidth=2, label=\"Trend line\")\n",
    "\n",
    "# X-axis setup\n",
    "plt.xticks(ticks=x, labels=df[\"Text\"], rotation=90, fontsize=8)\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.ylabel(\"TTR (Type-Token Ratio)\")\n",
    "plt.title(\"Type-Token Ratios of Speeches (Bar Graph, Color-coded by President + Trend Line)\")\n",
    "\n",
    "# Add legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=color_map[pres]) for pres in unique_presidents]\n",
    "plt.legend(handles + [plt.Line2D([0], [0], color=\"k\", linestyle=\"--\")],\n",
    "           list(unique_presidents) + [\"Trend line\"],\n",
    "           bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ff733",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
